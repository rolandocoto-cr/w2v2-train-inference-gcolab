{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train Wav2Vec2 model using Miniconda\n",
        "\n",
        "Rolando Coto-Solano (rolando.a.coto.solano@dartmouth.edu)<br>\n",
        "Last update: 20260114"
      ],
      "metadata": {
        "id": "hFMAApTDJVVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare preliminary data"
      ],
      "metadata": {
        "id": "MdJA1l_0H36C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime; print(datetime.now().time())"
      ],
      "metadata": {
        "id": "H6726qxT-96Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==================================================\n",
        "# Which files do you want to process?\n",
        "#==================================================\n",
        "\n",
        "currentSandbox = \"sandbox-user\" # Please type sandbox-user or all-wavs\n",
        "installationFolder = \"202506-ood-asr\"\n",
        "\n",
        "runId = \"01\"\n",
        "desiredTrainEpochs = 21\n",
        "\n",
        "trainFile = \"ood-wav2vec2-train.csv\"\n",
        "validFile = \"ood-wav2vec2-valid.csv\"\n",
        "testFile = \"ood-wav2vec2-test.csv\"\n",
        "\n",
        "asrLang = \"ood\"\n",
        "\n",
        "# N-grams for the KenLM n-gram model\n",
        "ngrams = 4"
      ],
      "metadata": {
        "id": "wyxuRJ7-Cmy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "id": "AkhrCSvtDBCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetPath = \"/content/drive/MyDrive/\"+installationFolder+\"/\" + currentSandbox + \"/\"\n",
        "\n",
        "csvTrain = datasetPath + trainFile\n",
        "csvValid = datasetPath + validFile\n",
        "csvTest = datasetPath + testFile\n",
        "corpusFile = datasetPath + trainFile.replace(\"-train.csv\",\"-corpus.txt\")\n",
        "\n",
        "filenameKenlmModel = \"lm-\" + asrLang + \"-\" + str(ngrams) + \".arpa\"\n",
        "filenameCorrectKenlmModel = filenameKenlmModel.replace(\".arpa\", \"-correct.arpa\")\n",
        "\n",
        "folderLogFiles = datasetPath + \"logs-wav2vec2-res/\"\n",
        "folderModelFiles = \"/content/wav2vec2-large-xlsr/\"\n",
        "\n",
        "condition = \"wav2vec2\"\n",
        "\n",
        "outputPrefix = asrLang + \"-\" + condition\n",
        "transferModelPath = \"\""
      ],
      "metadata": {
        "id": "yivSCc86Nj6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(folderLogFiles)\n",
        "print(folderModelFiles)"
      ],
      "metadata": {
        "id": "Eu3ny44YN2na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages"
      ],
      "metadata": {
        "id": "WebgV0_TH1rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://rcweb.dartmouth.edu/RCoto/tocc-asr-workshop-202506/train-wav2vec2lm-miniconda-202505.py"
      ],
      "metadata": {
        "id": "j2bI6yYDJJKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env PYTHONPATH="
      ],
      "metadata": {
        "id": "nm1cyB23H1eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-py310_25.3.1-1-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "metadata": {
        "id": "t_K4WjowH1be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda --version # should return 4.12.0\n",
        "!python --version"
      ],
      "metadata": {
        "id": "EF-wGjMbH1Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "conda install --channel defaults conda python=3.10 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "metadata": {
        "id": "mI8LoviPWtmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path\n",
        "\n",
        "_ = (sys.path\n",
        "        .append(\"/usr/local/lib/python3.10/site-packages\"))"
      ],
      "metadata": {
        "id": "E1ItqICLISKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=====================================================\n",
        "# Installing packages\n",
        "# This should take up to 9~13 minutes\n",
        "#=====================================================\n",
        "\n",
        "!conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main\n",
        "!conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r\n",
        "\n",
        "!conda install -c conda-forge numpy==1.23.5 -y\n",
        "!conda install -c conda-forge datasets==2.15.0 -y\n",
        "!conda install -c conda-forge transformers==4.28.0 -y\n",
        "!conda install -c conda-forge pandas==1.5.3 -y\n",
        "!conda install -c conda-forge pyctcdecode==0.3.0 -y\n",
        "!conda install -c conda-forge librosa==0.11.0 -y\n",
        "!conda install -c conda-forge typing==3.7.4.3 -y\n",
        "!conda install -c conda-forge statistics==1.0.3.5 -y\n",
        "!conda install -c conda-forge huggingface_hub==0.21.4 -y\n",
        "!conda install -c conda-forge kenlm\n",
        "!pip install https://github.com/kpu/kenlm/archive/master.zip\n",
        "!pip install torch==1.11.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "!pip install jiwer==3.1.0"
      ],
      "metadata": {
        "id": "nhi7iBVRId3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install KenLM\n",
        "#!apt install libboost-all-dev libeigen3-dev\n",
        "#!wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz\n",
        "#!mkdir -p kenlm/build\n",
        "#!cd kenlm/build && cmake .. && make -j2\n",
        "\n",
        "!rm -rf /content/kenlm  # Remove old directory completely\n",
        "!apt-get update\n",
        "!apt install -y libboost-all-dev libeigen3-dev build-essential\n",
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz\n",
        "!mkdir -p kenlm/build\n",
        "!cd kenlm/build && /usr/bin/cmake .. && make -j2"
      ],
      "metadata": {
        "id": "f8xOPub-RsY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make KenLM"
      ],
      "metadata": {
        "id": "1IduAoTvGTeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#==================================================================\n",
        "# Create corpus\n",
        "#==================================================================\n",
        "\n",
        "import csv\n",
        "\n",
        "def createCorpus(file1, file2, file3, fileOutput):\n",
        "    sentences = []\n",
        "\n",
        "    for path in [file1, file2, file3]:\n",
        "        with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            for row in reader:\n",
        "                sentence = row.get('sentence')\n",
        "                if sentence is not None:\n",
        "                    sentences.append(sentence)\n",
        "\n",
        "    with open(fileOutput, 'w', encoding='utf-8') as outfile:\n",
        "        outfile.write('\\n'.join(sentences))\n",
        "\n",
        "createCorpus(csvTrain, csvValid, csvTest, corpusFile)"
      ],
      "metadata": {
        "id": "LjhnOhMZCjmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kenlm/build/bin/lmplz -o {str(ngrams)} <\"{corpusFile}\" > \"{filenameKenlmModel}\""
      ],
      "metadata": {
        "id": "UBM6hUSpCjjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(filenameKenlmModel, \"r\") as read_file, open(filenameCorrectKenlmModel, \"w\") as write_file:\n",
        "  has_added_eos = False\n",
        "  for line in read_file:\n",
        "    if not has_added_eos and \"ngram 1=\" in line:\n",
        "      count=line.strip().split(\"=\")[-1]\n",
        "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
        "    elif not has_added_eos and \"<s>\" in line:\n",
        "      write_file.write(line)\n",
        "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
        "      has_added_eos = True\n",
        "    else:\n",
        "      write_file.write(line)"
      ],
      "metadata": {
        "id": "hHb_55rlCjgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -20 {filenameCorrectKenlmModel}"
      ],
      "metadata": {
        "id": "81Dner6yHQik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "avyUKO3nNNui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train-wav2vec2lm-miniconda-202505.py $asrLang $csvTrain $csvValid $csvTest $filenameCorrectKenlmModel $folderLogFiles $folderModelFiles $runId $desiredTrainEpochs $outputPrefix"
      ],
      "metadata": {
        "id": "BpuwzmATHQga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Results"
      ],
      "metadata": {
        "id": "mNd5p2FVmNSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look for the trained checkpoints\n",
        "\n",
        "import os\n",
        "\n",
        "sub_checkpoints = [name for name in os.listdir(folderModelFiles) if os.path.isdir(os.path.join(folderModelFiles, name))]\n",
        "checkpoints = []\n",
        "for f in sub_checkpoints:\n",
        "  if (\"checkpoint\" in f and \"ipynb\" not in f):\n",
        "    checkpoints.append(os.path.join(folderModelFiles, f))\n",
        "\n",
        "checkpointNums = []\n",
        "for ch in checkpoints:\n",
        "  checkpointNums.append(int(ch.split(\"-\")[-1]))\n",
        "checkpointNums.sort()\n",
        "checkpoints.sort()\n",
        "\n",
        "print(checkpointNums)\n",
        "print(checkpoints)\n",
        "\n",
        "lastcheckpoint = max(checkpointNums)\n",
        "print(\"Last checkpoint: \" + str(lastcheckpoint))"
      ],
      "metadata": {
        "id": "wDm50rn2WVUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model onto Google Colab"
      ],
      "metadata": {
        "id": "_UG_goutlkD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Which checkpoint do you want to save?\n",
        "\n",
        "# You can select specific checkpoint to save.\n",
        "# However, each of them is more than 3GB\n",
        "saveCheckpoints = [str(lastcheckpoint)]"
      ],
      "metadata": {
        "id": "NiMmRfBolisC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Erase previous models\n",
        "modelFolder = datasetPath + \"wav2vec2-model\"\n",
        "!rm -r {modelFolder}\n",
        "!mkdir {modelFolder}\n",
        "\n",
        "# Save new model\n",
        "\n",
        "for s in saveCheckpoints:\n",
        "\n",
        "  originFolder = folderModelFiles + \"checkpoint-\" + s\n",
        "  destinationFolder = datasetPath + \"wav2vec2-model/checkpoint-\" + s\n",
        "  !cp -r $originFolder $destinationFolder\n",
        "\n",
        "!cp {folderModelFiles}preprocessor_config.json {datasetPath}wav2vec2-model/\n",
        "!cp {folderModelFiles}special_tokens_map.json {datasetPath}wav2vec2-model/\n",
        "!cp {folderModelFiles}tokenizer_config.json {datasetPath}wav2vec2-model/\n",
        "!cp {folderModelFiles}vocab.json {datasetPath}wav2vec2-model/\n",
        "\n",
        "!cp /content/{filenameCorrectKenlmModel} {datasetPath}wav2vec2-model/\n",
        "!cp /content/{filenameKenlmModel} {datasetPath}wav2vec2-model/\n",
        "!cp {corpusFile} {datasetPath}wav2vec2-model/"
      ],
      "metadata": {
        "id": "HEe9kM1Cmvs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime; print(datetime.now().time())"
      ],
      "metadata": {
        "id": "gEN3cT3u_J_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import runtime\n",
        "#runtime.unassign()"
      ],
      "metadata": {
        "id": "NUxtuI2t-mpQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}