{"cells":[{"cell_type":"markdown","metadata":{"id":"aYu9YaXxXSrP"},"source":["# Transcribe a recording from the user using Wav2Vec2\n","Rolando Coto-Solano (Rolando.A.Coto.Solano@dartmouth.edu)<br>\n","Dartmouth College. Last update: 20250601\n","\n","The program takes two main inputs:\n","\n","* `currentSandbox`: The name of the sandbox you are using. The defaults are {sandbox-user and all-wavs}, but you can use whichever you specified during the installation.<br>\n","* `installationFolder`: The folder where the ASR sandboxes are contained. The default value is `202506-ood-asr`, but you should use the one you specified during the installation.<br>\n","\n","The program takes another input:\n","\n","* `modelCheckpointToUse`: The name of the folder that has the checkpoint for the transcription model. The default is `checkpoint-1200`, but you should check which one you saved by going to the `wav2vec2-model` folder.<br>\n","\n","This code loads a language model trained using [XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2) and uses it to transcribe recordings. The code is based on [Fine-tuning XLS-R for Multi-Lingual ASR with ðŸ¤— Transformers](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) by [Patrick von Platen](https://huggingface.co/patrickvonplaten).\n","\n","This file is useful to make a \"live demo\" of the transcription models."]},{"cell_type":"markdown","source":["## (1) Script preparation"],"metadata":{"id":"vAZmfJZw6fYR"}},{"cell_type":"code","source":["#=================================================\n","# If the computer tells you to \"restart session\",\n","# please restart it and run this box again.\n","#=================================================\n","\n","!pip install numpy==1.25.0"],"metadata":{"id":"E4aRyq27X5-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Environmental variables\n","currentSandbox = \"sandbox-user\"             # Please type sandbox-user or all-wavs\n","installationFolder = \"202506-ood-asr\"\n","\n","modelCheckpointToUse = \"checkpoint-1200\"      # You can type \"checkpoint-1200\" to use the model you trained\n","\n","# Use GPU for processing? If you select \"no\", then the system will use the slower CPU processing\n","useGPU = \"yes\""],"metadata":{"id":"xtyhGrmX6kZK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount the Google Drive onto the virtual computer\n","\n","from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"metadata":{"id":"sHxsh-o_Ctjk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EkcABL-hXgUK"},"source":["## (2) Model installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hggC77hXOlB"},"outputs":[],"source":["#=============================================================\n","# Installs software for ASR model\n","# Takes about 3~4 minutes\n","# You only need to run this once per session\n","#=============================================================\n","\n","#%%capture\n","!pip install datasets\n","!pip install transformers\n","!pip install librosa\n","!pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html  > /dev/null\n","\n","from transformers import Wav2Vec2ForCTC\n","from transformers import Wav2Vec2Processor\n","\n","import glob\n","import torch\n","import torchaudio\n","import pandas as pd\n","from datasets import Dataset\n","from transformers import Wav2Vec2FeatureExtractor\n","\n","#=============================================================\n","# Determine type of processing\n","#=============================================================\n","\n","typeProcessor = \"cuda\"\n","if (useGPU == \"no\"): typeProcessor = \"cpu\"\n","\n","#=============================================================\n","# Downloads ASR model for CIM\n","#=============================================================\n","\n","!mkdir /content/wav2vec2-model\n","!cp /content/drive/MyDrive/{installationFolder}/{currentSandbox}/wav2vec2-model/*.* /content/wav2vec2-model\n","!mkdir /content/wav2vec2-model/checkpoint\n","!cp /content/drive/MyDrive/{installationFolder}/{currentSandbox}/wav2vec2-model/{modelCheckpointToUse}/*.* /content/wav2vec2-model/checkpoint\n","\n","pathCheckpoint = \"wav2vec2-model/checkpoint\"\n","model = Wav2Vec2ForCTC.from_pretrained(pathCheckpoint).to(typeProcessor)\n","processor = Wav2Vec2Processor.from_pretrained(\"wav2vec2-model\")"]},{"cell_type":"markdown","metadata":{"id":"Rm4P7im1YP4-"},"source":["## (3) Audio Decoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptBxwzZiYcLl"},"outputs":[],"source":["# Minimum duration of segments that the computer should transcribe\n","minDurationOfFile = 100 #ms\n","\n","\n","#=============================================================\n","# Create dummy CSV file\n","#=============================================================\n","\n","output = \"path,sentence\\nsample-recording.wav,kia orana\"\n","f = open(\"sample-recording.csv\", \"w\")\n","f.write(output)\n","f.close()\n","\n","\n","#=============================================================\n","# Convert model orthography to human orthography\n","#=============================================================\n","\n","def convertToHumanOrthography(inputString):\n","\n","  outputString = inputString\n","\n","  # Replace the output transcription with orthographic output\n","  orthOrigin = ['ax', 'ex', 'ix', 'ox', 'ux', 'q']\n","  orthTarget = ['Ä', 'Ä“', 'Ä«', 'Å', 'Å«', 'êžŒ']\n","  for i in range(0,len(orthOrigin)): outputString = outputString.replace(orthOrigin[i], orthTarget[i])\n","\n","  return outputString\n","\n","\n","#=============================================================\n","# Function to run inference of new files\n","#=============================================================\n","\n","def runInference ():\n","\n","  # Load models\n","  #pathCheckpoint = \"cim-checkpoint-lrec/checkpoint\"\n","  #model = Wav2Vec2ForCTC.from_pretrained(pathCheckpoint).to(\"cuda\")\n","  #processor = Wav2Vec2Processor.from_pretrained(\"cim-checkpoint-lrec\")\n","\n","  # Convert audio file to array\n","  def speech_file_to_array_fn(batch):\n","      speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n","      batch[\"speech\"] = speech_array[0].numpy()\n","      batch[\"sampling_rate\"] = sampling_rate\n","      batch[\"target_text\"] = batch[\"sentence\"]\n","      return batch\n","\n","  # Prepare batch processing of files\n","  def prepare_dataset(batch):\n","      # check that all files have the correct sampling rate\n","      assert (\n","          len(set(batch[\"sampling_rate\"])) == 1\n","      ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n","\n","      batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n","\n","      with processor.as_target_processor():\n","          batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n","      return batch\n","\n","  # Load CSV file with audio files to be transcribed\n","  dataTest = pd.read_csv(\"sample-recording.csv\")\n","  common_voice_test = Dataset.from_pandas(dataTest)\n","\n","  # Extract features from audio files\n","  feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n","  common_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names)\n","  common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names, batch_size=8, num_proc=4, batched=True)\n","\n","  # Process audio files\n","  input_dict = processor(common_voice_test[0][\"input_values\"], return_tensors=\"pt\", padding=True)\n","  logits = model(input_dict.input_values.to(typeProcessor)).logits\n","  pred_ids = torch.argmax(logits, dim=-1)[0]\n","\n","  # Decode audio files\n","  predictedText = convertToHumanOrthography(processor.decode(pred_ids))\n","\n","  return predictedText"]},{"cell_type":"markdown","metadata":{"id":"5FOfEB9mVRsS"},"source":["## (4) Inference: Transcribing new files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WIJ2nQcVU2g"},"outputs":[],"source":["#=============================================================\n","# Code to record audio from Colab Notebook\n","# Takes about 30 seconds\n","#=============================================================\n","\n","!apt-get install libsox-fmt-all libsox-dev sox > /dev/null\n","!python -m pip install git+https://github.com/facebookresearch/WavAugment.git > /dev/null\n","!pip install ffmpeg-python > /dev/null\n","\n","import torchaudio\n","\n","# code from https://ricardodeazambuja.com/deep_learning/2019/03/09/audio_and_video_google_colab/\n","from IPython.display import HTML, Audio\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","import numpy as np\n","import io\n","import ffmpeg\n","import tempfile\n","import pathlib\n","\n","\n","AUDIO_HTML = \"\"\"\n","<script>\n","var my_div = document.createElement(\"DIV\");\n","var my_p = document.createElement(\"P\");\n","var my_btn = document.createElement(\"BUTTON\");\n","var t = document.createTextNode(\"Press to start recording\");\n","\n","my_btn.appendChild(t);\n","//my_p.appendChild(my_btn);\n","my_div.appendChild(my_btn);\n","document.body.appendChild(my_div);\n","\n","var base64data = 0;\n","var reader;\n","var recorder, gumStream;\n","var recordButton = my_btn;\n","\n","var handleSuccess = function(stream) {\n","  gumStream = stream;\n","  var options = {\n","    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n","    mimeType : 'audio/webm;codecs=opus'\n","    //mimeType : 'audio/webm;codecs=pcm'\n","  };\n","  //recorder = new MediaRecorder(stream, options);\n","  recorder = new MediaRecorder(stream);\n","  recorder.ondataavailable = function(e) {\n","    var url = URL.createObjectURL(e.data);\n","    var preview = document.createElement('audio');\n","    preview.controls = true;\n","    preview.src = url;\n","    document.body.appendChild(preview);\n","\n","    reader = new FileReader();\n","    reader.readAsDataURL(e.data);\n","    reader.onloadend = function() {\n","      base64data = reader.result;\n","      //console.log(\"Inside FileReader:\" + base64data);\n","    }\n","  };\n","  recorder.start();\n","  };\n","\n","recordButton.innerText = \"Recording... press to stop\";\n","\n","navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n","\n","\n","function toggleRecording() {\n","  if (recorder && recorder.state == \"recording\") {\n","      recorder.stop();\n","      gumStream.getAudioTracks()[0].stop();\n","      recordButton.innerText = \"Saving the recording... pls wait!\"\n","  }\n","}\n","\n","// https://stackoverflow.com/a/951057\n","function sleep(ms) {\n","  return new Promise(resolve => setTimeout(resolve, ms));\n","}\n","\n","var data = new Promise(resolve=>{\n","//recordButton.addEventListener(\"click\", toggleRecording);\n","recordButton.onclick = ()=>{\n","toggleRecording()\n","\n","sleep(2000).then(() => {\n","  // wait 2000ms for the data to be available...\n","  // ideally this should use something like await...\n","  //console.log(\"Inside data:\" + base64data)\n","  resolve(base64data.toString())\n","\n","});\n","\n","}\n","});\n","\n","</script>\n","\"\"\"\n","\n","def get_audio():\n","  display(HTML(AUDIO_HTML))\n","  data = eval_js(\"data\")\n","  binary = b64decode(data.split(',')[1])\n","\n","  process = (ffmpeg\n","    .input('pipe:0')\n","    .output('pipe:1', format='wav')\n","    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n","  )\n","  output, err = process.communicate(input=binary)\n","\n","  riff_chunk_size = len(output) - 8\n","  # Break up the chunk size into four bytes, held in b.\n","  q = riff_chunk_size\n","  b = []\n","  for i in range(4):\n","      q, r = divmod(q, 256)\n","      b.append(r)\n","\n","  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n","  riff = output[:4] + bytes(b) + output[8:]\n","\n","  with tempfile.TemporaryDirectory() as tmpdirname:\n","    path = pathlib.Path(tmpdirname) / 'tmp.wav'\n","    with open(path, 'wb') as f:\n","       f.write(riff)\n","    with open('temp.wav', 'wb') as f:\n","       f.write(riff)\n","\n","    x, sr = torchaudio.load(path)\n","\n","  return x, sr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CASLM0sTYfjf"},"outputs":[],"source":["# GET READY!\n","# When you run the next box, the computer will say that it's listening to you.\n","# I recommend that you say something simple. For example:\n","#\n","# Kia orana!\n","#\n","# It means \"Hello\" or \"How are you?\"\n","# After you said it, click on \"Press to stop\". Once the \"play\" button (on the left) stops rotating, that's\n","# when your recording will be ready and you can continue to the next step. (The button will keep saying\n","# \"saving the recording... pls wait!\". Don't pay attention to that. Pay attention to the play button. If\n","# the rotating line has stopped, then you're good).\n","#\n","# The system will make many mistakes because it's only trained on 16 minutes on audio.\n","# As the system learns from more data, the transcriptions will get better."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExVFZ6biYrCU"},"outputs":[],"source":["#==========================================================================\n","# This will open a button to record audio.\n","# It will save the file as temp.wav\n","# Once you see a play button, this means that the file is saved correctly.\n","#==========================================================================\n","\n","%rm /content/temp.wav\n","%cd /content/\n","x, sr = get_audio()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7PPKEwloYskv"},"outputs":[],"source":["#==========================================================================\n","# Convert the audio file to mono (one channel)\n","# and downgrade its quality to 16KHz\n","#==========================================================================\n","\n","!ffmpeg -y -i temp.wav -ac 1 -ar 16000 temp-sample-recording.wav\n","!rm temp.wav\n","!mv temp-sample-recording.wav sample-recording.wav"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byTFp4jWbzew"},"outputs":[],"source":["#==========================================================================\n","# Transcribe recording\n","#==========================================================================\n","\n","predictedText = runInference()"]},{"cell_type":"code","source":["#==========================================================================\n","# Print the transcription of the recording\n","#==========================================================================\n","\n","print(\"Prediction:\")\n","print(predictedText)"],"metadata":{"id":"828i8YwTD5Zb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46gH9bUSby2A"},"outputs":[],"source":["#==========================================================================\n","# Play recording\n","#==========================================================================\n","\n","import IPython\n","IPython.display.Audio('sample-recording.wav') # This is required on Google Colab due to compatibility issues"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}