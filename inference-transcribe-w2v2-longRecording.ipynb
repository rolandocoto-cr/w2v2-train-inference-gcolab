{"cells":[{"cell_type":"markdown","metadata":{"id":"Xzt21_ZXIxZG"},"source":["# Transcribe WAV file using Wav2Vec2\n","Rolando Coto-Solano (Rolando.A.Coto.Solano@dartmouth.edu)<br>\n","Dartmouth College. Last update: 20250601\n","\n","The program takes four main inputs:\n","\n","* `audioFileName`: The WAV, MP3 or MP4 file that you wish to transcribe. It needs to be in the `audiofiles-to-transcribe` folder of the sandbox you will use.<br>\n","* `currentSandbox`: The name of the sandbox you are using. The defaults are {sandbox-user and all-wavs}, but you can use whichever you specified during the installation.<br>\n","* `installationFolder`: The folder where the ASR sandboxes are contained. The default value is `202506-ood-asr`, but you should use the one you specified during the installation.<br>\n","\n","The program also takes the following inputs:\n","\n","* `modelCheckpointToUse`: The name of the folder that has the checkpoint for the transcription model. The default is `checkpoint-1200`, but you should check which one you saved by going to the `wav2vec2-model` folder.<br>\n","* `minDurationOfFile `: Minimum duration of a file that should be transcribed The default is 100 milliseconds.<br>\n","* `maxWavDuration`: The maximum duration for a recording to be allowed into the Wav2Vec2 data. Wav2Vec2's CUDA memory crashed when processing long files. The default is 15 seconds; this is the maximum duration where I can guarantee that the Colab memory won't crash.<br>\n","\n","The program takes the audio file. It then (1) splits the large audio file into smaller chunks using Silero-VAD, and (2) runs each of these through the transcription model. It then (3) saves these results into a TSV file in the folder `tsv-outputs`. This file can then be imported into ELAN."]},{"cell_type":"markdown","metadata":{"id":"34ZPUhA0qV0I"},"source":["## (1) File preparation\n","\n","You need to run this for every new file you process"]},{"cell_type":"code","source":["#=================================================\n","# If the computer tells you to \"restart session\",\n","# please restart it and run this box again.\n","#=================================================\n","\n","!pip install numpy==1.25.0"],"metadata":{"id":"C0y0WY1e2vnP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLGHlzbbWjH8"},"outputs":[],"source":["# The file should be in the folder audiofiles-to-transcribe (inside of your sandbox)\n","audioFileName = \"kia-orana-rehearsal.mp4\"\n","\n","# Environmental variables\n","currentSandbox = \"sandbox-user\"    # Please type sandbox-user or all-wavs\n","installationFolder = \"202506-ood-asr\"\n","\n","# Model variables\n","modelCheckpointToUse = \"checkpoint-1200\"       # You can type \"full-cim-model\" to use the pretrained model\n","                                              # Or you can type someting like \"checkpoint-1200\" to use the model you trained\n","# Minimum duration of segments that the computer should transcribe\n","minDurationOfFile = 100 #ms\n","\n","# Maximum permissible duration of segments. Wav2Vec2's CUDA memory might crash when processing long files\n","maxWavDuration = 15    # Seconds\n","\n","# Use GPU for processing? If you select \"no\", then the system will use the slower CPU processing\n","useGPU = \"yes\""]},{"cell_type":"code","source":["# ================================================================\n","# Mount the Google Drive onto the virtual computer\n","# ================================================================\n","\n","from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"metadata":{"id":"3UtXe3skxQLl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## (2) Model preparation\n","\n","You only need to run this once per session. It should take about 2 minutes."],"metadata":{"id":"L4XrumM14DEP"}},{"cell_type":"code","source":["!pip install silero-vad\n","from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n","vadmodel = load_silero_vad()"],"metadata":{"id":"nP_2cztfyWOv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#=============================================================\n","# Determine type of processing\n","#=============================================================\n","\n","typeProcessor = \"cuda\"\n","if (useGPU == \"no\"): typeProcessor = \"cpu\"\n","\n","#=============================================================\n","# Downloads ASR model for CIM\n","#=============================================================\n","\n","!mkdir /content/wav2vec2-model\n","!cp /content/drive/MyDrive/{installationFolder}/{currentSandbox}/wav2vec2-model/*.* /content/wav2vec2-model\n","!mkdir /content/wav2vec2-model/checkpoint\n","!cp /content/drive/MyDrive/{installationFolder}/{currentSandbox}/wav2vec2-model/{modelCheckpointToUse}/*.* /content/wav2vec2-model/checkpoint\n","\n","pathCheckpoint = \"/content/wav2vec2-model/checkpoint\"\n","modelPath = \"/content/wav2vec2-model\"\n","\n","#model = Wav2Vec2ForCTC.from_pretrained(pathCheckpoint).to(typeProcessor)\n","#processor = Wav2Vec2Processor.from_pretrained(\"wav2vec2-model\")"],"metadata":{"id":"293XiNlv0_Ra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","def findKenLMModel(folder_path):\n","    for entry in os.listdir(folder_path):\n","        if entry.endswith('-correct.arpa') and os.path.isfile(os.path.join(folder_path, entry)):\n","            return entry\n","    return \"-1\"  # if no such file is found\n","\n","filenameCorrectKenlmModel = findKenLMModel(modelPath + \"/\")\n","filenameCorrectKenlmModel = modelPath + \"/\" + filenameCorrectKenlmModel"],"metadata":{"id":"KXK7H12fb20H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## (3) Split audio file into bits\n","\n","You need to run this again for every file you process."],"metadata":{"id":"8mZk_xws0tGv"}},{"cell_type":"code","source":["def extractStartEndTimes(filepath):\n","    startTimes = []\n","    endTimes = []\n","\n","    with open(filepath, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            # Strip newline and split by tabs\n","            parts = line.strip().split('\\t')\n","\n","            if len(parts) < 3:\n","                continue\n","\n","            try:\n","                start = float(parts[0])\n","                end = float(parts[1])\n","                startTimes.append(start)\n","                endTimes.append(end)\n","            except ValueError:\n","                # If conversion to float fails, skip that line\n","                continue\n","\n","    return startTimes, endTimes"],"metadata":{"id":"SDsaQ-gjQDfK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def countDigits(number):\n","  count=0\n","  while(number>0):\n","    count=count+1\n","    number=number//10\n","  return(count)\n","\n","def addZerosInFrontOfNumber(number, total):\n","\n","  lenNum = len(str(number))\n","  lenTotal = len(str(total))\n","\n","  zerosToAdd = lenTotal-lenNum\n","\n","  stringZeros = \"\"\n","  for i in range(0,zerosToAdd): stringZeros = stringZeros + \"0\"\n","  retNum = stringZeros + str(number)\n","  return retNum"],"metadata":{"id":"1uRRPfcYQZZK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def findSegmentIndex(timepoints, starttime, endtime):\n","    # Convert timepoints string to a sorted list of floats\n","    points = sorted([float(tp) for tp in timepoints.split(',')])\n","\n","    # Find the segment index where starttime and endtime correspond exactly\n","    # Segment 0: [0, points[0])\n","    # Segment i: [points[i-1], points[i])\n","\n","    # Check if starttime is 0 (start of segment 0) or matches points\n","    # Since segments are defined between these points, the input start and end should match\n","    # one of the segment boundaries\n","\n","    # We look for the segment where:\n","    # segment i means interval [points[i-1], points[i])\n","\n","    # For segment 0: interval [0, points[0])\n","    for i, point in enumerate(points):\n","        if i == 0:\n","            seg_start = 0.0\n","        else:\n","            seg_start = points[i-1]\n","        seg_end = point\n","\n","        # Check if the starttime and endtime match this segment\n","        if starttime == seg_start and endtime == seg_end:\n","            return i\n","\n","    # If no match found, optionally return None or error\n","    return -1"],"metadata":{"id":"8a0hsDJzQh6w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from decimal import Decimal\n","import os\n","\n","def fixIntervals(audio_path, file_path, maxDuration):\n","    # This list will hold all the intervals after processing\n","    intervals = []\n","\n","    totalLines = 0\n","\n","    # Open the file and read lines\n","    with open(file_path, 'r') as f:\n","        for line in f:\n","            totalLines = totalLines + 1\n","            parts = line.strip().split(\",\")\n","            if len(parts) != 2:\n","                continue\n","            start, end = float(parts[0]), float(parts[1])\n","\n","            # Check if the duration exceeds 15 seconds\n","            while end - start > maxDuration:\n","                # Add an interval with a maximum of 15 seconds\n","                intervals.append((start, start + maxDuration))\n","                start += maxDuration + 0.001  # Increment start for the next interval\n","\n","            # Add the final interval (which is <= 15 seconds)\n","            intervals.append((start, end))\n","\n","    # Sort intervals based on the start time\n","    intervals.sort()\n","\n","    output = \"\"\n","\n","    lineCounter = 0\n","    filenames = []\n","    # Print the processed and sorted intervals\n","    for interval in intervals:\n","        lineCounter = lineCounter + 1\n","        nameAudio = audio_path.replace(\".wav\", \"-\" + str(addZerosInFrontOfNumber(lineCounter,totalLines)) + \".wav\")\n","        filenames.append(nameAudio)\n","        #output = output + str(round(Decimal(interval[0]),3)) + \"\\t\" + interval[1] + \"\\n\"\n","        output = output + str(round(Decimal(interval[0]),3)) + \"\\t\" + str(round(Decimal(interval[1]),3)) + \"\\t\" + nameAudio + \"\\n\"\n","        #print(f\"{interval[0]:.3f} {interval[1]:.3f}\")\n","\n","    file_path = file_path.replace(\".csv\", \".tsv\")\n","    f = open(file_path, \"w\")\n","    f.write(output)\n","    f.close()\n","\n","    sampleCSV = \"path,sentence\\n\"\n","    for f in filenames:\n","      sampleCSV = sampleCSV + f + \", \\n\"\n","    sampleCSV = sampleCSV[:-1]\n","\n","    folderPath = os.path.dirname(file_path)\n","    samplePath = folderPath + \"/sample.csv\"\n","    #print(samplePath)\n","    #print(sampleCSV)\n","    f = open(samplePath, \"w\")\n","    f.write(sampleCSV)\n","    f.close()\n","\n","\n","def leaveOnlyLastThreeColsOfTSV(inPath, outPath):\n","    try:\n","        with open(inPath, 'r', encoding='utf-8') as infile:  # Open the input file\n","            lines = infile.readlines()  # Read all lines\n","\n","        processed_lines = []\n","        for line in lines:\n","            columns = line.strip().split('\\t')  # Split the line into columns using tab as the delimiter\n","            if len(columns) >= 3:  # Ensure there are at least 3 columns to keep\n","                # Keep only the last three columns (columns[2], columns[3], columns[4])\n","                processed_lines.append(','.join(columns[2:]) + '\\n')  # Rejoin and append to the list\n","\n","        with open(outPath, 'w', encoding='utf-8') as outfile:  # Open the output file for writing\n","            outfile.writelines(processed_lines)  # Write the processed lines to the output file\n","\n","        print(\"File processed successfully from {} to {}.\".format(inPath, outPath))\n","\n","    except IOError as e:\n","        print(\"An IOError occurred: {}\".format(e))\n","    except Exception as e:\n","        print(\"An unexpected error occurred: {}\".format(e))\n"],"metadata":{"id":"8S9-Z2sUTE57"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","\n","#=============================================================\n","# Erase previous files and get new audio file\n","#=============================================================\n","\n","# Erase previous files\n","%cd /content/\n","%rm *.wav >/dev/null 2>&1\n","%rm *.WAV >/dev/null 2>&1\n","%rm *.mp3 >/dev/null 2>&1\n","%rm *.MP3 >/dev/null 2>&1\n","%rm *.mp4 >/dev/null 2>&1\n","%rm *.MP4 >/dev/null 2>&1\n","%rm *.csv >/dev/null 2>&1\n","%rm *.CSV >/dev/null 2>&1\n","%rm *.tsv >/dev/null 2>&1\n","%rm *.TSV >/dev/null 2>&1\n","%rm *.txt >/dev/null 2>&1\n","%rm *.TXT >/dev/null 2>&1\n","\n","folderWithAudioFiles = \"/content/drive/MyDrive/\"+installationFolder+\"/\"+currentSandbox+\"/audiofiles-to-transcribe/\"\n","!cp {folderWithAudioFiles}{audioFileName} .\n","\n","#=============================================================\n","# Get proper path to audio file\n","#=============================================================\n","\n","# get filenames of wave files in the remote server\n","path1 = r'/content/*.wav'\n","path2 = r'/content/*.WAV'\n","path3 = r'/content/*.mp3'\n","path4 = r'/content/*.MP3'\n","path5 = r'/content/*.mp4'\n","path6 = r'/content/*.MP4'\n","path7 = r'/content/*.mov'\n","path8 = r'/content/*.MOV'\n","\n","files = []\n","files = glob.glob(path1) + glob.glob(path2) + glob.glob(path3) + glob.glob(path4) + glob.glob(path5) + glob.glob(path6) + glob.glob(path7) + glob.glob(path8)\n","\n","# get name of the first file\n","if (len(files) == 0):\n","  print(\"=== ERROR: THERE ARE NO AUDIO FILES IN THE REMOTE SERVER ===\")\n","else:\n","  wavfile = \"\"\n","  annotfile = \"\"\n","  for i in range(0,1): wavfile = files[i].replace(\"/content/\",\"\")\n","  print(\"File to split: \" + wavfile)\n","  print(\"Annotation file: \" + annotfile)\n","\n","fileExtensionOrig = wavfile[-3:]\n","fileExtension = fileExtensionOrig.lower()\n","origFilename = wavfile\n","\n","\n","#=============================================================\n","# Convert file from mp3/mp4 to wav\n","#=============================================================\n","\n","if (fileExtension == \"mp3\" or fileExtension == \"mp4\"):\n","  wavfile = origFilename.replace(origFilename[-3:],\"\") + \"wav\"\n","  !ffmpeg -i $origFilename -acodec pcm_u8 $wavfile\n","  print(wavfile)\n","\n","\n","#=============================================================\n","# Downgrade WAV file to the right ASR format (e.g. 16K)\n","#=============================================================\n","\n","!ffmpeg -y -i $wavfile -ac 1 -ar 16000 temp-$wavfile\n","!rm $wavfile\n","!mv temp-$wavfile $wavfile\n","\n","#=============================================================\n","# Find voice regions\n","#=============================================================\n","\n","wav = read_audio(wavfile)\n","speech_timestamps = get_speech_timestamps(\n","  wav,\n","  vadmodel,\n","  return_seconds=True,  # Return speech timestamps in seconds (default is samples)\n",")\n","\n","#=============================================================\n","# Write voice regions into a file\n","#=============================================================\n","\n","output = \"\"\n","\n","linePrefix = \"tiername\\tspeakername\\t\"\n","\n","for t in speech_timestamps:\n","  #print(t['start'])\n","  output += linePrefix + str(t['start']) + \"\\t\" + str(t['end']) + \"\\t \\n\"\n","output = output[:-1]\n","\n","with open(\"voice-regions.txt\", \"w\") as file: file.write(output)\n","\n","#=============================================================\n","# Make sure there aren't any regions that are longer\n","# than the memory limit\n","#=============================================================\n","\n","leaveOnlyLastThreeColsOfTSV(\"voice-regions.txt\",\"temp-regions.txt\")\n","fixIntervals(wavfile, \"temp-regions.txt\", maxWavDuration)"],"metadata":{"id":"hbDcp4O650we"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["timeStart, timeEnd = extractStartEndTimes('temp-regions.txt')"],"metadata":{"id":"yYKxbi0SQKAy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#============================================================\n","# Separate the big file into smaller wav files\n","#============================================================\n","\n","%mkdir wavs\n","\n","filenames = []\n","print(len(timeStart))\n","\n","\n","points = []\n","for start, end in zip(timeStart, timeEnd):\n","  i = len(points)-1\n","  #tempName = \"out\" + addZerosToNumber(i+1,len(timeStart)) + \".wav\"\n","  #tempName2 = \"out\" + addZerosToNumber(i+2,len(timeStart)) + \".wav\"\n","  if points and points[-1] == start:\n","    points.append(end)\n","    #filenames.append(tempName)\n","  else:\n","    points.append(start)\n","    points.append(end)\n","    #filenames.append(tempName2)\n","\n","pointSeq = ','.join(str(t) for t in points)\n","inFileName = wavfile\n","\n","zeros = countDigits(len(timeStart))\n","outFileName = \"wavs/out\" + f'-%0{zeros}d.wav'\n","!ffmpeg -y -i \"$inFileName\" -f segment -ac 1 -ar 16000 -async 1 -segment_times $pointSeq $outFileName"],"metadata":{"id":"0Ebh-RTuQJ92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outwavNames = []\n","\n","for i in range(0,len(timeStart)):\n","  #print(timeStart[i])\n","  #print(timeEnd[i])\n","  segmentIndex = findSegmentIndex(pointSeq, timeStart[i], timeEnd[i])\n","  outwavNames.append(\"/content/wavs/out-\" + addZerosInFrontOfNumber(str(segmentIndex),len(timeStart)) + \".wav\")\n","\n","print(outwavNames)"],"metadata":{"id":"9MYCivorQJ7N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#outputName = \"tempWav\\tstart\\tend\\n\"\n","outputName = \"\"\n","transcribeFile = \"path,sentence\\n\"\n","\n","for i in range(0,len(outwavNames)):\n","  outputName += str(timeStart[i]) + \"\\t\" + str(timeEnd[i]) + \"\\t\" + outwavNames[i] + \"\\n\"\n","  transcribeFile += outwavNames[i] + \", \\n\"\n","outputName = outputName[:-1]\n","transcribeFile = transcribeFile[:-1]\n","\n","with open(\"sample.csv\", 'w', encoding='utf-8') as f: f.write(transcribeFile)\n","with open(\"files-and-times.txt\", 'w', encoding='utf-8') as f: f.write(outputName)"],"metadata":{"id":"em1ZhNP1QJ4s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## (4) Install decoding packages\n","\n","This should take about 2 minutes. You only need to run it once per session."],"metadata":{"id":"bh1Jjl0MZ4QC"}},{"cell_type":"code","source":["!pip install datasets==2.15.0\n","!pip install transformers==4.28.0\n","!pip install pandas==1.5.3\n","!pip install pyctcdecode==0.3.0\n","!pip install librosa==0.11.0\n","!pip install jiwer==3.1.0\n","!pip install https://github.com/kpu/kenlm/archive/master.zip"],"metadata":{"id":"by7HnZWeZ6ij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","currentDateAndTime = datetime.now()\n","startTime = str(currentDateAndTime)\n","\n","import os\n","\n","import transformers\n","import datasets\n","import torch\n","\n","import numpy\n","import numpy as np\n","\n","import pandas\n","import pandas as pd\n","\n","from datasets import load_dataset, load_metric\n","from datasets import Dataset\n","from datasets import ClassLabel\n","from dataclasses import dataclass, field\n","from typing import Any, Dict, List, Optional, Union\n","\n","from transformers import Wav2Vec2ForCTC\n","from transformers import Wav2Vec2Processor\n","from transformers import Wav2Vec2CTCTokenizer\n","from transformers import Wav2Vec2FeatureExtractor\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","from transformers import Wav2Vec2ProcessorWithLM\n","\n","import pyctcdecode\n","from pyctcdecode import build_ctcdecoder\n","\n","import random\n","import re\n","import json\n","\n","import torchaudio\n","import librosa\n","\n","from jiwer import wer\n","import statistics\n","\n","from multiprocessing import get_context\n","import kenlm"],"metadata":{"id":"6r5QBE6ChRpE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## (5) Decode the file and transcribe it"],"metadata":{"id":"jrS81BYUYDq5"}},{"cell_type":"code","source":["#============================================================================\n","# Load CSV files and prepare dataset\n","#============================================================================\n","\n","dataTest = pd.read_csv(\"sample.csv\")\n","\n","dataTest.head()\n","\n","common_voice_test = Dataset.from_pandas(dataTest)\n","common_voice_test_transcription = Dataset.from_pandas(dataTest)\n","\n","chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\â€œ\\%\\â€˜\\â€\\ï¿½]'\n","\n","def remove_special_characters(batch):\n","    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n","    return batch\n","\n","common_voice_test = common_voice_test.map(remove_special_characters)\n","\n","def extract_all_chars(batch):\n","  all_text = \" \".join(batch[\"sentence\"])\n","  vocab = list(set(all_text))\n","  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n","\n","vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)\n","\n","#============================================================================\n","# Load model\n","#============================================================================\n","\n","model = Wav2Vec2ForCTC.from_pretrained(pathCheckpoint).to(typeProcessor)\n","processor = Wav2Vec2Processor.from_pretrained(modelPath)\n","\n","vocab_dict = processor.tokenizer.get_vocab()\n","sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n","\n","tempDecoder = build_ctcdecoder(\n","\tlabels=list(sorted_vocab_dict.keys()),\n","\tkenlm_model_path=filenameCorrectKenlmModel,\n",")\n","\n","processor_with_lm = Wav2Vec2ProcessorWithLM(\n","\tfeature_extractor=processor.feature_extractor,\n","\ttokenizer=processor.tokenizer,\n","\tdecoder=tempDecoder\n",")\n","\n","#============================================================================\n","# Preprocess data\n","#============================================================================\n","\n","def speech_file_to_array_fn(batch):\n","    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n","    batch[\"speech\"] = speech_array[0].numpy()\n","    batch[\"sampling_rate\"] = sampling_rate\n","    batch[\"target_text\"] = batch[\"sentence\"]\n","    return batch\n","\n","common_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names)\n","\n","def resample(batch):\n","    batch[\"speech\"] = librosa.resample(np.asarray(batch[\"speech\"]), 48_000, 16_000)\n","    batch[\"sampling_rate\"] = 16_000\n","    return batch\n","\n","def prepare_dataset(batch):\n","    # check that all files have the correct sampling rate\n","    assert (\n","        len(set(batch[\"sampling_rate\"])) == 1\n","    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n","\n","    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0], padding=True).input_values\n","\n","    with processor.as_target_processor():\n","        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n","    return batch\n","\n","common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names, batch_size=8, num_proc=4, batched=True)\n","\n","#===============================================================================\n","# Transcribe files\n","#===============================================================================\n","\n","prediction = []\n","predictionLM = []\n","reference = []\n","paths = []\n","\n","for i in range(0,len(common_voice_test)):\n","\n","\tinput_dict = processor_with_lm(common_voice_test[i][\"input_values\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n","\tlogits = model(input_dict.input_values.to(typeProcessor)).logits\n","\tpred_ids = torch.argmax(logits, dim=-1)[0]\n","\n","\t#print(\"Prediction:\")\n","\twith get_context(\"fork\").Pool(processes=4) as pool:\n","\t\tpredictionlm = processor_with_lm.batch_decode(logits.cpu().detach().numpy(), pool).text[0]\n","\tpredictionLM.append(predictionlm)\n","\tprint(predictionlm)\n","\t#if (i == 0): print(\"LM Prediction: \" + str(predictionlm))\n","\t#print(\"LM Prediction: \" + str(predictionlm))\n","\n","\tpath = common_voice_test_transcription[i][\"path\"]\n","\tpath = path.split(\"/\")\n","\tpath = path[-1]\n","\tpaths.append(path)\n","\n","#===============================================================================\n","# Save transcriptions\n","#===============================================================================\n","\n","tsvTimes = \"files-and-times.txt\"\n","\n","linesTSV = []\n","\n","with open(tsvTimes, 'r') as file:\n","\t# Read each line and add it to the list\n","\tlinesTSV = file.readlines()\n","linesTSV = [line.strip() for line in linesTSV]\n","\n","outputTranscriptions = \"start\\tend\\ttranscription\\n\"\n","\n","for l in linesTSV:\n","\n","\tl = l.split(\"\\t\")\n","\t#print(l[2])\n","\ttsvWaveChunk = os.path.basename(l[2])\n","\n","\tfor i in range(0,len(predictionLM)):\n","\t\t#print(paths[i])\n","\t\tif (tsvWaveChunk == paths[i]):\n","\t\t\toutputTranscriptions = outputTranscriptions + l[0] + \"\\t\" + l[1] + \"\\t\" + predictionLM[i] + \"\\n\"\n","\n","outputTranscriptions = outputTranscriptions[:-1]\n","\n","tsvOutputputFilename = os.path.basename(wavfile.replace(\".wav\",\".tsv\"))\n","\n","file = open(tsvOutputputFilename, 'w')\n","file.write(outputTranscriptions)\n","file.close()\n","\n","print(\"\\nThe results are stored in: \\n\" + tsvOutputputFilename)"],"metadata":{"id":"wDqU_Y-JQJzY"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"collapsed_sections":["L4XrumM14DEP","8mZk_xws0tGv","bh1Jjl0MZ4QC"],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}